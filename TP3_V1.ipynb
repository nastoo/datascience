{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Import de NTLK et du corpus inaugural\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bible csv\n",
    "bible_df = pd.read_csv('bible.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats descriptives sur le dataframe\n",
    "bible_df.rename(columns={\n",
    "    'b': 'book_id', \n",
    "    'c': 'chapter_id',\n",
    "    'v': 'verse_id',\n",
    "    't': 'text'\n",
    "    }, inplace=True)\n",
    "\n",
    "bible_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, clean !\n",
    "\n",
    "# Use lambda to apply the function to each row of the DataFrame\n",
    "def remove_all_punctuation(text):\n",
    "   return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def remove_partial_punctuation(text): # except .!?\n",
    "   return re.sub(r'[^\\w\\s.!?]', ' ', text)\n",
    "\n",
    "def carriage_return(text):\n",
    "   return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def remove_double_space(text): # remove when more than 2 spaces\n",
    "   return re.sub(r'[ ]{2,}', ' ', text)\n",
    "\n",
    "def remove_empty_strings(text):\n",
    "    return list(filter(None, text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "   text = tokenize(text)\n",
    "   filtered_words = [word for word in text if word.lower() not in stopwords.words('english')]\n",
    "   return filtered_words\n",
    "\n",
    "def tokenize(text):\n",
    "   return text.split(' ')\n",
    "\n",
    "def join(text):\n",
    "   return ' '.join(text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    WNlemma = nltk.WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for token in text:\n",
    "        lemmatized.append(WNlemma.lemmatize(token))\n",
    "    return lemmatized\n",
    "\n",
    "def remove_numbers(text):\n",
    "    numbers = '0123456789'\n",
    "    for number in numbers:\n",
    "        text = text.replace(number, '')\n",
    "    return text\n",
    "\n",
    "def remove_short_words(word_list):\n",
    "    return [word for word in word_list if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text without stopwords\n",
    "bible_df['cleaned'] = bible_df['text'] \\\n",
    "   .apply(lambda x: remove_stopwords(x)) \\\n",
    "   .apply(lambda x: join(x)) \\\n",
    "   .apply(lambda x: carriage_return(x)) \\\n",
    "   .apply(lambda x: remove_double_space(x)) \\\n",
    "   .apply(lambda x: x.lower()) \\\n",
    "   .apply(lambda x: remove_all_punctuation(x)) \\\n",
    "   .apply(lambda x: remove_numbers(x)) \\\n",
    "   .apply(lambda x: tokenize(x)) \\\n",
    "   .apply(lambda x: remove_empty_strings(x)) \\\n",
    "   .apply(lambda x: lemmatize(x)) \\\n",
    "   .apply(lambda x: remove_short_words(x))\n",
    "   \n",
    "# Test cleaning efficiency printing the dataframe\n",
    "bible_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_df.plot(x='book_id', y='chapter_id', kind='scatter')\n",
    "# book_id = représente le livre de la bible (1 = Genèse, 2 = Exode, 3= Lévitique, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ceci est un test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistiques du texte (avec visualisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add descriptive columns\n",
    "bible_df['word_count'] = bible_df['cleaned'].apply(lambda x: len(' '.join(x).split()))\n",
    "bible_df['unique_words'] = bible_df['cleaned'].apply(lambda x: len(set(x)))\n",
    "bible_df['avg_word_length'] = bible_df['cleaned'].apply(lambda x: sum(len(word) for word in x) / len(x) if len(x) > 0 else 0) \n",
    "bible_df['sentence_count'] = bible_df['text'].apply(lambda x: x.count('.') + x.count('!') + x.count('?'))\n",
    "\n",
    "bible_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de mots par livre de la bible\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graph 1: Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bible_df['book_id'], bible_df['word_count'])\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Word Count by Book')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top des livres par nombre de mots (top 20) pour la lisibilité\n",
    "top_10_books = bible_df.groupby('book_id')['word_count'].sum().nlargest(20)\n",
    "top_10_books\n",
    "top_10_books.plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Top 20 Books by Word Count (66 at all)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de mots par livre de la bible \n",
    "plt.figure(figsize=(10, 6))\n",
    "bible_df_sorted = bible_df.sort_values('avg_word_length')\n",
    "plt.bar(bible_df_sorted['book_id'], bible_df_sorted['avg_word_length'])\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Average Word Length')\n",
    "plt.title('Average Word Length by Book (Descending Order)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de plusieurs fonctions de nettoyage des données, et application de ces dernières sur différentes colonnes du dataframe, correspondant à un niveau de nettoyage différent qui sera utile pour les prochaines étapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use lambda to apply the function to each row of the DataFrame\n",
    "def remove_all_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def remove_partial_punctuation(text): # except .!?\n",
    "    return re.sub(r'[^\\w\\s.!?]', ' ', text)\n",
    "\n",
    "def carriage_return(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def remove_double_space(text): # remove when more than 2 spaces\n",
    "    return re.sub(r'[ ]{2,}', ' ', text)\n",
    "\n",
    "def remove_empty_strings(text):\n",
    "     return list(filter(None, text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = tokenize(text)\n",
    "    filtered_words = [word for word in text if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def join(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "def lemmatize(text):\n",
    "     WNlemma = nltk.WordNetLemmatizer()\n",
    "     lemmatized = []\n",
    "     for token in text:\n",
    "          lemmatized.append(WNlemma.lemmatize(token))\n",
    "     return lemmatized\n",
    "\n",
    "def remove_numbers(text):\n",
    "     numbers = '0123456789'\n",
    "     for number in numbers:\n",
    "          text = text.replace(number, '')\n",
    "     return text\n",
    "\n",
    "def remove_single_letter_words(text):\n",
    "    filtered_words = [word for word in text if len(word) > 1]\n",
    "    return filtered_words\n",
    "\n",
    "bible_df_cleaned = bible_df.copy()\n",
    "\n",
    "# Text without stopwords\n",
    "bible_df_cleaned['text_without_stopwords'] = bible_df_cleaned['text'] \\\n",
    "    .apply(lambda x: remove_stopwords(x)) \\\n",
    "    .apply(lambda x: join(x)) \\\n",
    "    .apply(lambda x: carriage_return(x)) \\\n",
    "    .apply(lambda x: remove_double_space(x))\n",
    "\n",
    "# Clean text partially for later\n",
    "bible_df_cleaned['text_partially_cleaned'] = bible_df_cleaned['text_without_stopwords'] \\\n",
    "    .apply(lambda x: remove_partial_punctuation(x)) \\\n",
    "    .apply(lambda x: remove_double_space(x)) \\\n",
    "    .apply(lambda x: x.lower()) \\\n",
    "\n",
    "# Advanced cleaning\n",
    "bible_df_cleaned['text'] = bible_df_cleaned['text_partially_cleaned'] \\\n",
    "    .apply(lambda x: remove_all_punctuation(x)) \\\n",
    "    .apply(lambda x: remove_numbers(x)) \\\n",
    "    .apply(lambda x: remove_double_space(x)) \\\n",
    "    .apply(lambda x: tokenize(x)) \\\n",
    "    .apply(lambda x: remove_single_letter_words(x)) \\\n",
    "    .apply(lambda x: remove_empty_strings(x)) \\\n",
    "    .apply(lambda x: lemmatize(x))\n",
    "\n",
    "# Test cleaning efficiency printing the dataframe\n",
    "bible_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cleaning efficiency printing the first row\n",
    "# cela permet de comparer les différentes étapes de nettoyage\n",
    "\n",
    "print('Cleaned text: \\n' + str(bible_df_cleaned['text'][0]))\n",
    "print('Partially cleaned text (without puncutation except points):\\n' + str(bible_df_cleaned['text_partially_cleaned'][0]))\n",
    "print('Text without stopwords and carriage return: \\n' + str(bible_df_cleaned['text_without_stopwords'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des mots les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "combined_common_words = []\n",
    "\n",
    "# Print the most common sorted words\n",
    "for index, text in enumerate(bible_df_cleaned['text']):\n",
    "    vocab = Counter(text)\n",
    "    most_common_words = vocab.most_common()\n",
    "    select_freq_words = [w for w in most_common_words if len(w[0]) > 3 and w[1] > 1]\n",
    "    sorted_select_freq_words = sorted(select_freq_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    bible_df_cleaned.loc[index, 'most_common_words'] = str(sorted_select_freq_words)\n",
    "   \n",
    "    # Append word only to combined_common_words \n",
    "    for word in sorted_select_freq_words:\n",
    "        combined_common_words.append(word[0])\n",
    "\n",
    "combined_common_words = sorted(set(combined_common_words))\n",
    "print(combined_common_words)\n",
    "print('Unique repeated words count: ' + str(len(combined_common_words)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un dataframe contenant les mots les plus fréquents\n",
    "Ce n'était pas forcément demandé, et cela rejoint un peu le BOW qui sera fait plus tard d'une manière plus simple, mais cela permet de visualiser les mots les plus fréquents et de les comparer entre eux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create an empty dataframe\n",
    "word_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row in the cleaned text column\n",
    "for index, text in enumerate(bible_df_cleaned['text']):\n",
    "    # Count the frequency of each word in the text\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # Create a dictionary with the word as the key and its frequency as the value\n",
    "    word_dict = {word: count for word, count in word_counts.items()}\n",
    "    \n",
    "    # Create a temporary dataframe with the word frequencies\n",
    "    temp_df = pd.DataFrame.from_dict(word_dict, orient='index', columns=[f'Phrase {index+1}'])\n",
    "    \n",
    "    # Concatenate the temporary dataframe with the word dataframe\n",
    "    word_df = pd.concat([word_df, temp_df], axis=1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "word_df = word_df.fillna(0)\n",
    "\n",
    "# Sort the dataframe by the sum of word frequencies in descending order\n",
    "word_df = word_df.reindex(word_df.sum().sort_values(ascending=False).index, axis=1)\n",
    "\n",
    "# Print the word dataframe\n",
    "word_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

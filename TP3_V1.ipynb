{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Import de NTLK et du corpus inaugural\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bible csv\n",
    "bible_df = pd.read_csv('bible.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats descriptives sur le dataframe\n",
    "bible_df.rename(columns={\n",
    "    'b': 'book_id', \n",
    "    'c': 'chapter_id',\n",
    "    'v': 'verse_id',\n",
    "    't': 'text'\n",
    "    }, inplace=True)\n",
    "\n",
    "bible_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, clean !\n",
    "\n",
    "# Use lambda to apply the function to each row of the DataFrame\n",
    "def remove_all_punctuation(text):\n",
    "   return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def remove_partial_punctuation(text): # except .!?\n",
    "   return re.sub(r'[^\\w\\s.!?]', ' ', text)\n",
    "\n",
    "def carriage_return(text):\n",
    "   return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def remove_double_space(text): # remove when more than 2 spaces\n",
    "   return re.sub(r'[ ]{2,}', ' ', text)\n",
    "\n",
    "def remove_empty_strings(text):\n",
    "    return list(filter(None, text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "   text = tokenize(text)\n",
    "   filtered_words = [word for word in text if word.lower() not in stopwords.words('english')]\n",
    "   return filtered_words\n",
    "\n",
    "def tokenize(text):\n",
    "   return text.split(' ')\n",
    "\n",
    "def join(text):\n",
    "   return ' '.join(text)\n",
    "\n",
    "def lemmatize(text):\n",
    "    WNlemma = nltk.WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for token in text:\n",
    "        lemmatized.append(WNlemma.lemmatize(token))\n",
    "    return lemmatized\n",
    "\n",
    "def remove_numbers(text):\n",
    "    numbers = '0123456789'\n",
    "    for number in numbers:\n",
    "        text = text.replace(number, '')\n",
    "    return text\n",
    "\n",
    "def remove_short_words(word_list):\n",
    "    return [word for word in word_list if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text without stopwords\n",
    "bible_df['cleaned'] = bible_df['text'] \\\n",
    "   .apply(lambda x: remove_stopwords(x)) \\\n",
    "   .apply(lambda x: join(x)) \\\n",
    "   .apply(lambda x: carriage_return(x)) \\\n",
    "   .apply(lambda x: remove_double_space(x)) \\\n",
    "   .apply(lambda x: x.lower()) \\\n",
    "   .apply(lambda x: remove_all_punctuation(x)) \\\n",
    "   .apply(lambda x: remove_numbers(x)) \\\n",
    "   .apply(lambda x: tokenize(x)) \\\n",
    "   .apply(lambda x: remove_empty_strings(x)) \\\n",
    "   .apply(lambda x: lemmatize(x)) \\\n",
    "   .apply(lambda x: remove_short_words(x))\n",
    "   \n",
    "bible_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistiques du texte (avec visualisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add descriptive columns\n",
    "bible_df['word_count'] = bible_df['cleaned'].apply(lambda x: len(' '.join(x).split()))\n",
    "bible_df['unique_words'] = bible_df['cleaned'].apply(lambda x: len(set(x)))\n",
    "bible_df['avg_word_length'] = bible_df['cleaned'].apply(lambda x: sum(len(word) for word in x) / len(x) if len(x) > 0 else 0) \n",
    "bible_df['sentence_count'] = bible_df['text'].apply(lambda x: x.count('.') + x.count('!') + x.count('?'))\n",
    "\n",
    "bible_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top des livres par nombre de mots (top 20) pour la lisibilité\n",
    "top_books = bible_df.groupby('book_id')['word_count'].sum().nlargest(66)\n",
    "average_word_count = top_books.mean()\n",
    "\n",
    "top_books.plot(kind='bar', figsize=(10, 6))\n",
    "plt.axhline(average_word_count, color='red', linestyle='--', label='Average')\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Top Books by Word Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Nombres de mots par livre de la bible \n",
    "plt.figure(figsize=(10, 6))\n",
    "bible_df_sorted = bible_df.sort_values('avg_word_length', ascending=False)  # Sort in descending order\n",
    "plt.bar(bible_df_sorted['book_id'], bible_df_sorted['avg_word_length'])\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Average Word Length')\n",
    "plt.title('Average Word Length by Book')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de plusieurs fonctions de nettoyage des données, et application de ces dernières sur différentes colonnes du dataframe, correspondant à un niveau de nettoyage différent qui sera utile pour les prochaines étapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use lambda to apply the function to each row of the DataFrame\n",
    "def remove_all_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def remove_partial_punctuation(text): # except .!?\n",
    "    return re.sub(r'[^\\w\\s.!?]', ' ', text)\n",
    "\n",
    "def carriage_return(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def remove_double_space(text): # remove when more than 2 spaces\n",
    "    return re.sub(r'[ ]{2,}', ' ', text)\n",
    "\n",
    "def remove_empty_strings(text):\n",
    "     return list(filter(None, text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = tokenize(text)\n",
    "    filtered_words = [word for word in text if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def join(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "def lemmatize(text):\n",
    "     WNlemma = nltk.WordNetLemmatizer()\n",
    "     lemmatized = []\n",
    "     for token in text:\n",
    "          lemmatized.append(WNlemma.lemmatize(token))\n",
    "     return lemmatized\n",
    "\n",
    "def remove_numbers(text):\n",
    "     numbers = '0123456789'\n",
    "     for number in numbers:\n",
    "          text = text.replace(number, '')\n",
    "     return text\n",
    "\n",
    "def remove_single_letter_words(text):\n",
    "    filtered_words = [word for word in text if len(word) > 1]\n",
    "    return filtered_words\n",
    "\n",
    "bible_df_cleaned = bible_df.copy()\n",
    "\n",
    "# Text without stopwords\n",
    "# bible_df_cleaned['text_without_stopwords'] = bible_df_cleaned['text'] \\\n",
    "#     .apply(lambda x: remove_stopwords(x)) \\\n",
    "#     .apply(lambda x: join(x)) \\\n",
    "#     .apply(lambda x: carriage_return(x)) \\\n",
    "#     .apply(lambda x: remove_double_space(x))\n",
    "\n",
    "# # Clean text partially for later\n",
    "# bible_df_cleaned['text_partially_cleaned'] = bible_df_cleaned['text_without_stopwords'] \\\n",
    "#     .apply(lambda x: remove_partial_punctuation(x)) \\\n",
    "#     .apply(lambda x: remove_double_space(x)) \\\n",
    "#     .apply(lambda x: x.lower()) \\\n",
    "\n",
    "# # Advanced cleaning\n",
    "# bible_df_cleaned['text'] = bible_df_cleaned['text_partially_cleaned'] \\\n",
    "#     .apply(lambda x: remove_all_punctuation(x)) \\\n",
    "#     .apply(lambda x: remove_numbers(x)) \\\n",
    "#     .apply(lambda x: remove_double_space(x)) \\\n",
    "#     .apply(lambda x: tokenize(x)) \\\n",
    "#     .apply(lambda x: remove_single_letter_words(x)) \\\n",
    "#     .apply(lambda x: remove_empty_strings(x)) \\\n",
    "#     .apply(lambda x: lemmatize(x))\n",
    "\n",
    "# Test cleaning efficiency printing the dataframe\n",
    "bible_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cleaning efficiency printing the first row\n",
    "# cela permet de comparer les différentes étapes de nettoyage\n",
    "\n",
    "print('Cleaned text: \\n' + str(bible_df_cleaned['text'][0]))\n",
    "print('Cleaned text: \\n' + str(bible_df_cleaned['cleaned'][0]))\n",
    "# print('Partially cleaned text (without puncutation except points):\\n' + str(bible_df_cleaned['text_partially_cleaned'][0]))\n",
    "# print('Text without stopwords and carriage return: \\n' + str(bible_df_cleaned['text_without_stopwords'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des mots les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Concatenate all the cleaned text into a single string\n",
    "all_text = ' '.join(bible_df_cleaned['cleaned'].sum())\n",
    "\n",
    "# Count the frequency of each word in the text\n",
    "word_counts = Counter(all_text.split())\n",
    "\n",
    "# Get the top 10 most common words\n",
    "top_words = word_counts.most_common(10)\n",
    "\n",
    "# Print the top words\n",
    "for word, count in top_words:\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of verse in the bible\n",
    "print(\"Number of verse in the bible : \", len(bible_df_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = len(bible_df_cleaned['text'].explode().unique())\n",
    "print(\"Number of unique words in the Bible:\", unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times \"Jesus\" appears in the Bible\n",
    "jesus_count = bible_df_cleaned['cleaned'].explode().str.count('jesus').sum()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of times 'Jesus' appears in the Bible:\", int(jesus_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "god_count = bible_df_cleaned['cleaned'].explode().str.count('god').sum()\n",
    "print(\"Number of times 'God' appears in the Bible:\", int(god_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group the dataframe by book_id and calculate the sum of occurrences for each word\n",
    "god_counts = bible_df_cleaned.groupby('book_id')['cleaned'].apply(lambda x: x.explode().str.count('god').sum())\n",
    "jesus_counts = bible_df_cleaned.groupby('book_id')['cleaned'].apply(lambda x: x.explode().str.count('jesus').sum())\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(god_counts.index, god_counts.values, label='God')\n",
    "plt.plot(jesus_counts.index, jesus_counts.values, label='Jesus')\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.title('Evolution of Word Occurrences (God and Jesus)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to include only books 1 to 37\n",
    "old_testament_df = bible_df_cleaned[(bible_df_cleaned['book_id'] >= 1) & (bible_df_cleaned['book_id'] <= 37)]\n",
    "\n",
    "# Count the occurrences of the word \"jesus\" in the filtered dataframe\n",
    "jesus_count = old_testament_df['text'].explode().str.count('jesus').sum()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of times 'Jesus' appears between books 1 and 37:\", jesus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Noms des apôtres\n",
    "apostles = ['Simon', 'Andrew', 'James', 'John', 'Philip', 'Bartholomew', 'Matthew', 'Thomas', 'James (son of Alphaeus)', 'Thaddaeus', 'Simon the Zealot', 'Judas Iscariot']\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les occurrences des noms des apôtres par livre\n",
    "apostle_counts = {apostle: [] for apostle in apostles}\n",
    "\n",
    "# Parcourir les apôtres\n",
    "for apostle in apostles:\n",
    "    # Calculer les occurrences pour chaque apôtre à partir du livre 35\n",
    "    counts = bible_df_cleaned[bible_df_cleaned['book_id'] >= 38].groupby('book_id')['cleaned'].apply(lambda x: x.explode().str.count(apostle.lower()).sum())\n",
    "    apostle_counts[apostle] = counts.values\n",
    "\n",
    "# Créer un graphique en ligne pour chaque apôtre\n",
    "plt.figure(figsize=(15, 8))\n",
    "for apostle in apostles:\n",
    "    plt.plot(counts.index, apostle_counts[apostle], label=apostle)\n",
    "\n",
    "plt.xlabel('ID du Livre')\n",
    "plt.ylabel('Nombre d\\'Occurrences')\n",
    "plt.title('Évolution des Occurrences des Noms des Apôtres à partir du Livre 38 de la Bible')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.Transformation des données\n",
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Perform part-of-speech tagging on the \"cleaned\" column\n",
    "bible_df_cleaned['pos_tags'] = bible_df_cleaned['cleaned'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "# Print only the \"pos_tags\" column\n",
    "print(bible_df_cleaned['pos_tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the NLTK data for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Perform part-of-speech tagging on the \"cleaned\" column\n",
    "bible_df_cleaned['pos_tags'] = bible_df_cleaned['cleaned'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "# Extract all the nouns from the \"pos_tags\" column\n",
    "nouns = [word for tags in bible_df_cleaned['pos_tags'] for word, pos in tags if pos.startswith('NN')]\n",
    "\n",
    "# Count the frequency of each noun\n",
    "noun_counts = Counter(nouns)\n",
    "\n",
    "# Select the top 10 most frequent nouns\n",
    "top_nouns = noun_counts.most_common(10)\n",
    "\n",
    "# Extract the nouns and their frequencies\n",
    "nouns, frequencies = zip(*top_nouns)\n",
    "\n",
    "# Plot the frequency of the top 10 nouns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(nouns, frequencies)\n",
    "plt.xlabel('Nouns')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Most Common Nouns (NN) in the Bible')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading resources\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Tokenization and POS tagging\n",
    "tokens = bible_df_cleaned['cleaned']\n",
    "pos_tags = bible_df_cleaned['pos_tags']\n",
    "\n",
    "print(tokens[0])\n",
    "print(pos_tags[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Perform Named Entity Recognition (NER)\n",
    "ner_tags = [nltk.ne_chunk(tags) for tags in pos_tags]\n",
    "\n",
    "# Print the NER tags\n",
    "for tags in ner_tags:\n",
    "    print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all uniques words and add them to a dataframe\n",
    "\n",
    "cleaned_corpus = bible_df_cleaned['cleaned'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(cleaned_corpus)\n",
    "\n",
    "cv_matrix1 = cv_matrix.toarray()\n",
    "cv_matrix1\n",
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names_out()\n",
    "# show document feature vectors\n",
    "bible_df_feature_vectors = pd.DataFrame(cv_matrix1, columns=vocab)\n",
    "\n",
    "bible_df_feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des bi-grammes les plus intéressants, avec une occurence tous textes confondus supérieure à 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Not working, need to sample bigrams randomly\n",
    "\n",
    "# bv = CountVectorizer(ngram_range=(2,2))\n",
    "# bv_matrix = bv.fit_transform(cleaned_corpus)\n",
    "# bv_matrix = bv_matrix.toarray()\n",
    "# vocab = bv.get_feature_names_out()\n",
    "# bible_df_bigram_trigram = pd.DataFrame(bv_matrix, columns=vocab)\n",
    "# bible_df_bigram_trigram.sum()\n",
    "# # Delete columns with less than 25 occurences everywhere \n",
    "# df_inaugural_corpus_bigram_trigram = bible_df_bigram_trigram.loc[:, (bible_df_bigram_trigram.sum(axis=0) >= 2)]\n",
    "\n",
    "# Add year column to the dataframe as index\n",
    "# bible_bigram_trigram.insert(0, 'year', bible_df_bigram_trigram['year'].values)\n",
    "# bible_bigram_trigram\n",
    "\n",
    "# # Plot the most common bigrams and trigrams by year\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.ylabel('Inaugural words count')\n",
    "# plt.xlabel('Year') \n",
    "# plt.title('Most common bigrams and trigrams by year of the inaugural discourses')\n",
    "# plt.subplots_adjust(bottom=0.15)\n",
    "# plt.xticks(rotation=90)\n",
    "# # make one plot for each bigram and trigram\n",
    "# for column in bible_bigram_trigram.drop(columns=['year']).columns:\n",
    "#     plt.plot(bible_bigram_trigram['year'], bible_df_bigram_trigram[column])\n",
    "\n",
    "# # add legend    \n",
    "# plt.legend(bible_bigram_trigram.drop(columns=['year']).columns, loc='upper left')\n",
    "\n",
    "\n",
    "# bible_bigram_trigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Concatenate all the cleaned text into a single string\n",
    "all_text = ' '.join(bible_df_cleaned['cleaned'].sum())\n",
    "\n",
    "# Create a CountVectorizer object with ngram_range=(2, 2)\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit the CountVectorizer on the concatenated text\n",
    "cv.fit([all_text])\n",
    "\n",
    "# Get the vocabulary (bigrams) and their counts\n",
    "vocab = cv.get_feature_names_out()\n",
    "counts = cv.transform([all_text]).toarray().flatten()\n",
    "\n",
    "# Filter the bigrams based on the occurrence threshold\n",
    "filtered_bigrams = [bigram for bigram, count in zip(vocab, counts) if count > 30]\n",
    "\n",
    "# Print the filtered bigrams\n",
    "print(filtered_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "# Reuse BOW from before\n",
    "feature_names = list(bible_df_feature_vectors.columns)\n",
    "# build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bible_df_feature_vectors, copy=True).indptr)\n",
    "df = 1 + df # adding 1 to smoothen idf later\n",
    "# show smoothened document frequencies\n",
    "pd.DataFrame([df], columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer().fit(bible_df_feature_vectors)\n",
    "len(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vect.transform(bible_df_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "sorted_tfidf_index = X_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la matrice TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(token_pattern=r'\\b\\w{8,}\\b')\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(bible_df_cleaned['text'])\n",
    "\n",
    "# Obtenir les noms des termes\n",
    "feature_names = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Créer une liste pour stocker les résultats de chaque livre\n",
    "tfidf_book_results = []\n",
    "\n",
    "# Parcourir chaque livre et obtenir les 3 termes avec les scores TF-IDF les plus élevés\n",
    "for idx, book_text in enumerate(bible_df_cleaned['text']):\n",
    "    # Obtenir le nom du livre à partir de l'index\n",
    "    book_name = idx + 1\n",
    "    \n",
    "    # Obtenir les scores TF-IDF pour chaque terme dans le livre\n",
    "    tfidf_scores = tfidf_matrix[idx, :].toarray()[0]\n",
    "    \n",
    "    # Créer une liste de tuples (terme, score TF-IDF) pour les termes de plus de 7 lettres\n",
    "    term_tfidf_list = [(term, round(score, 2)) for term, score in zip(feature_names, tfidf_scores) if len(term) > 6]\n",
    "\n",
    "    # Trier la liste par score TF-IDF décroissant et prendre les 3 premiers termes\n",
    "    sorted_tfidf_list = sorted(term_tfidf_list, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    # Ajouter les 3 termes avec leurs scores TF-IDF aux résultats\n",
    "    tfidf_book_results.append((book_name, sorted_tfidf_list))\n",
    "\n",
    "# Créer un dataframe à partir des résultats\n",
    "tfidf_df = pd.DataFrame(tfidf_book_results, columns=['Livre', 'Top 3 Termes TF-IDF'])\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with fit_transform\n",
    "# Initialize a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the complaints text to create embeddings\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(bible_df_cleaned['text'].apply(lambda x: \" \".join(x)))\n",
    " \n",
    "tfidf_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "# Initialize PCA and reduce dimensionality to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "reduced_tfidf = pca.fit_transform(tfidf_matrix.toarray())\n",
    " \n",
    "reduced_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test d'Elbow pour déterminer le nombre optimal de clusters\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Determine the optimal number of clusters using the Elbow method\n",
    "scores = []  # within-cluster sum of squares\n",
    "cluster_range = range(1, 10)  # test up to 10 clusters\n",
    " \n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=100)\n",
    "    kmeans.fit(reduced_tfidf)\n",
    "    scores.append(kmeans.inertia_)\n",
    " \n",
    "#Plot the Elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering with 3 clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=100)\n",
    "clusters = kmeans.fit_predict(reduced_tfidf)\n",
    " \n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_tfidf[:, 0], reduced_tfidf[:, 1], c=clusters, cmap='rainbow')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Clusters of Inaugural discourses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = bible_df_cleaned['text']\n",
    "y_km = kmeans.fit_predict(reduced_tfidf)\n",
    "df_km = pd.DataFrame({'statements' :X_train, 'topic_cluster' :y_km })\n",
    "print(df_km.groupby('topic_cluster').count())\n",
    "df_km.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## On crée notre dictionnaire des données\n",
    "id2word = corpora.Dictionary(bible_df_cleaned['text'])\n",
    "## On crée nos vecteurs avec Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in bible_df_cleaned['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 4\n",
    "# Build LDA model\n",
    "lda_model3 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                        random_state=100)\n",
    "# Print the Keyword in the 3 topics\n",
    "pprint(lda_model3.print_topics(num_topics))\n",
    "doc_lda3 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model3, texts=bible_df_cleaned['text'], dictionary=id2word, coherence='u_mass')\n",
    "coherence_model_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_model_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "# Visualisation of topics \n",
    "vis_data = gensimvis.prepare(lda_model3, corpus, id2word, sort_topics=False)\n",
    "pyLDAvis.save_html(vis_data, 'output_topics_LDA.html')\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A garder en plus\n",
    "# Choosing number of topics Round 2\n",
    "topics = list(range(1,20))\n",
    "coherences = []\n",
    "\n",
    "for no_topics in topics:\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=no_topics, random_state=100)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=bible_df_cleaned['text'], dictionary=id2word, coherence='u_mass')\n",
    "    coherences.append(coherence_model_lda.get_coherence())\n",
    "    print(coherence_model_lda.get_coherence())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Sentiment Analysis avec NLTK (from nltk.sentiment import sentimentIntensityAnalyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Convert the list of words to a string\n",
    "bible_df_cleaned['cleaned_text'] = bible_df_cleaned['cleaned'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate sentiment score for a given verse\n",
    "def get_sentiment_score(verse):\n",
    "    # Calculate sentiment score\n",
    "    sentiment_score = sia.polarity_scores(verse)['compound']\n",
    "    return sentiment_score\n",
    "\n",
    "# Apply the sentiment analysis function to the 'cleaned_text' column in your DataFrame\n",
    "bible_df_cleaned['sentiment_score'] = bible_df_cleaned['cleaned_text'].apply(get_sentiment_score)\n",
    "\n",
    "# Categorize sentiment based on the sentiment score\n",
    "bible_df_cleaned['sentiment'] = bible_df_cleaned['sentiment_score'].apply(lambda score: 'positive' if score > 0 else 'negative' if score < 0 else 'neutral')\n",
    "\n",
    "# Display the DataFrame with sentiment scores and categories\n",
    "print(bible_df_cleaned[['text', 'sentiment_score', 'sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for visualization\n",
    "\n",
    "visualization_df = pd.DataFrame({\n",
    "    'Verse': bible_df_cleaned['text'],\n",
    "    'Sentiment Score': bible_df_cleaned['sentiment_score'],\n",
    "    'Sentiment': bible_df_cleaned['sentiment']\n",
    "})\n",
    "\n",
    "visualization_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_verses = visualization_df[visualization_df['Sentiment'] == 'negative']\n",
    "negative_verses\n",
    "\n",
    "# exemple words negative : evil, wicked, hate, kill, death, sin, devil, hell, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the count of each sentiment category\n",
    "sentiment_counts = visualization_df['Sentiment'].value_counts()\n",
    "\n",
    "# Plot the sentiment counts\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_df['book_id'] = bible_df_cleaned['book_id']\n",
    "visualization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the count of positive and negative sentiments for each book_id\n",
    "sentiment_counts = visualization_df.groupby('book_id')['Sentiment'].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Determine if each book_id is positive or negative based on sentiment counts\n",
    "sentiment_counts['Book Sentiment'] = sentiment_counts.apply(lambda row: 'positive' if row['positive'] > row['negative'] else 'negative', axis=1)\n",
    "\n",
    "# Plot the book sentiments\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sentiment_counts.index, sentiment_counts['positive'], label='Positive')\n",
    "plt.bar(sentiment_counts.index, sentiment_counts['negative'], bottom=sentiment_counts['positive'], label='Negative')\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Sentiment Count')\n",
    "plt.title('Book Sentiments')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de voir les sentiments par livre de la bible (positif ou négatif)\n",
    "# si plus de verse positif que négatif alors le livre est positif (et inversement)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the count of positive and negative sentiments for each book_id\n",
    "sentiment_counts = visualization_df.groupby('book_id')['Sentiment'].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Determine if each book_id is positive or negative based on sentiment counts\n",
    "sentiment_counts['Book Sentiment'] = sentiment_counts.apply(lambda row: 'positive' if row['positive'] > row['negative'] else 'negative', axis=1)\n",
    "\n",
    "# Assign colors based on sentiment\n",
    "colors = ['green' if sentiment == 'positive' else 'red' for sentiment in sentiment_counts['Book Sentiment']]\n",
    "\n",
    "# Plot the book sentiments with assigned colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sentiment_counts.index, sentiment_counts['positive'], color=colors)\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Sentiment Count')\n",
    "plt.title('Book Sentiments')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

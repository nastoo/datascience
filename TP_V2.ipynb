{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "TP Groupe - Noé Duhamel, Guillaume Gatille & Nathan Stooss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_path = \"bible.csv\"\n",
    "df = pd.read_csv(df_path)\n",
    "print(\"Rows: \" + format(df.shape[0]))\n",
    "print(\"Columns: \" + format(df.shape[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Définissons la variable qui divisera l'ensemble des données en Nouveau et Ancien Testament."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['t'] = df['t'].astype('str')\n",
    "df.loc[df['b'] <= 39, 'testament'] = 'Old'\n",
    "df.loc[df['b'] > 39, 'testament'] = 'New'\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Stats descriptives sur le dataframe\n",
    "df.rename(columns={\n",
    "    'b': 'book_id', \n",
    "    'c': 'chapter_id',\n",
    "    'v': 'verse_id',\n",
    "    't': 'text'\n",
    "    }, inplace=True)\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nettoyage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Obtention de la liste des stopwords à supprimer (plus performante que la librairie de NLTK par défaut)\n",
    "words_to_delete = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "                   \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "                   'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                   'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', \n",
    "                   'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', \n",
    "                   'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', \n",
    "                   'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "                   'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "                   'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "                   'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n",
    "                   'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \n",
    "                   'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "                   \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
    "                   'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "                    , 'unto', 'thou', 'thee', 'ye', 'him', 'upon', 'say', 'me', 'hath', 'also',\"shouldn't\", \"wasn't\", \"weren't\", \n",
    "                    \"won't\", \"wouldn't\", \"unto\", \"thou\", \"thee\", \"ye\", \"him\", \"upon\", \"say\", \"me\", \"hath\", \"also\", \"ye\"\n",
    "                  ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Retirer les mots spécifiés du texte\n",
    "df['text_cleaned'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in words_to_delete]))\n",
    "\n",
    "# Retirer les chiffres\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\d+', '')\n",
    "\n",
    "# Retirer la ponctuation\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Retirer les espaces en trop\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "\n",
    "# Convertir text_cleaned en minuscule\n",
    "df['text_cleaned'] = df['text_cleaned'].str.lower()\n",
    "\n",
    "# Lemmatisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Export en CSV pour éviter le retraitement sur des ordinateurs moins performants\n",
    "df.to_csv('bible-cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test d'efficacité du nettoyage — Impression de la première ligne\n",
    "# Cela permet de comparer les différentes étapes de nettoyage\n",
    "print('Cleaned text: \\n' + str(df['text'][0]))\n",
    "print('Cleaned text: \\n' + str(df['text_cleaned'][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_to_convert = ['id', 'book_id', 'chapter_id', 'verse_id']\n",
    "df[columns_to_convert] = df[columns_to_convert].astype('int16')\n",
    "print(df.dtypes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_to_convert = ['text', 'testament', 'text_cleaned']\n",
    "df[columns_to_convert] = df[columns_to_convert].astype('string')\n",
    "print(df.dtypes)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Ajout des colonnes contenant des stats descriptives\n",
    "df['word_count'] = df['text_cleaned'].apply(lambda x: len(x.split()))\n",
    "df['unique_word_count'] = df['text_cleaned'].apply(lambda x: len(set(x.split())))\n",
    "df['sentence_count'] = df['text_cleaned'].apply(lambda x: len(sent_tokenize(x)))\n",
    "df['avg_word_length'] = df['text_cleaned'].apply(lambda x: np.mean([len(word) for word in x.split()])).round(2)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avec cet ensemble de données donné, on peux appliquer une analyse complète. Tout d'abord, on commence par un sujet intéressant : comment la longueur des versets évolue à travers les livres de la Bible. \n",
    "Pourquoi cette information peut-elle être utile ? Elle nous permet d'estimer approximativement quand les livres ont été écrits et la longueur des versets offre beaucoup de connaissances sur la culture à ce moment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "verses_column = 'verse_id'\n",
    "words_column = 'word_count'\n",
    "\n",
    "# Ajout de couleur\n",
    "color_1 = plt.cm.Blues(np.linspace(0.6, 1, 66))\n",
    "color_2 = plt.cm.Purples(np.linspace(0.6, 1, 66))\n",
    "\n",
    "# Regroupement par 'book_id' représentant les livres de la Bible.\n",
    "words_verses = df.groupby('book_id').agg({verses_column: 'count', words_column: 'sum'}).sort_values(by=verses_column, ascending=False)\n",
    "data1 = words_verses[verses_column]\n",
    "data2 = words_verses[words_column]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "x = np.arange(66)\n",
    "ax1 = plt.subplot(1, 1, 1)\n",
    "w = 0.3\n",
    "\n",
    "color = color_1\n",
    "plt.title('Nombre de mots vs nombre de versets')\n",
    "plt.xticks(x + w / 2, data1.index, rotation=-90)\n",
    "ax1.set_xlabel('Livres de la Bible')\n",
    "ax1.set_ylabel('Nombre de versets')\n",
    "ax1.bar(x, data1.values, color=color_1, width=w, align='center')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = color_2\n",
    "ax2.set_ylabel('Nombre de mots')\n",
    "ax2.bar(x + w, data2, color=color_2, width=w, align='center')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En visualisant l'évolution du nombre total de mots par livre, on peux  obtenir des informations sur la longueur relative des livres. Certains livres peuvent être plus longs que d'autres, ce qui peut refléter la complexité de leur contenu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Calcul du nombre total de mots par livre\n",
    "# total_words_by_book = df.groupby('book_id')['word_count'].sum().reset_index()\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# sns.lineplot(x='book_id', y='word_count', data=total_words_by_book, marker='o', color='green')\n",
    "# plt.title('Evolution of the Number of Words in Books')\n",
    "# plt.xlabel('Book ID')\n",
    "# plt.ylabel('Total Word Count')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Group the dataframe by book_id and calculate the sum of occurrences for each word\n",
    "verses_counts = df.groupby('book_id')['verse_id'].count()\n",
    "words_counts = df.groupby('book_id')['word_count'].sum()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(verses_counts.index, words_counts, width=0.8)\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Number of Words per Books')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En analysant graphiquement les données, on peut rapidement identifier les livres où les références à \"God\" et \"Jesus\" sont plus fréquentes, ainsi que les tendances générales au fil des différents IDs de livre. Ici Jésus apparait dans le New testament logiquement."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Regroupement des données par numéro de livre et calcul de la somme des occurrences pour chaque mot\n",
    "god_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('god').sum())\n",
    "jesus_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('jesus').sum())\n",
    "\n",
    "# Création d'un graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(god_counts.index, god_counts.values, label='God')\n",
    "plt.plot(jesus_counts.index, jesus_counts.values, label='Jesus')\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Nombre d\\'occurences')\n",
    "plt.title('Evolution des occurences des mots (God and Jesus)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce graphique facilite  l'identification de tendances générales dans l'utilisation de ces termes clés lié au pouvoir et facilitant ainsi une analyse thématique eventuel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Regroupement des données par numéro de livre et calcul de la somme des occurrences pour chaque mot\n",
    "lord_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('lord').sum())\n",
    "god_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('god').sum())\n",
    "king_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('king').sum())\n",
    "\n",
    "# Création d'un graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lord_counts.index, lord_counts.values, label='Lord')\n",
    "plt.plot(god_counts.index, god_counts.values, label='God')\n",
    "plt.plot(king_counts.index, king_counts.values, label='King')\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.title('Evolution of Word Occurrences (Lord, God, King)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "La représentation graphique des termes \"Holy,\" \"Son,\" et \"Father\" dans l'Ancien Testament et le NOUVEAU est utile car elle offre une vision  des thèmes théologiques ET permet une compréhension  rapide du vocabulaire utilisé.\n",
    "\n",
    "# Group the dataframe by book_id and calculate the sum of occurrences for each word\n",
    "holy_spirit_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('holy').sum())\n",
    "son_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('son').sum())\n",
    "father_counts = df.groupby('book_id')['text_cleaned'].apply(lambda x: x.explode().str.count('father').sum())\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(holy_spirit_counts.index, holy_spirit_counts.values, label='Holy')\n",
    "plt.plot(son_counts.index, son_counts.values, label='Son')\n",
    "plt.plot(father_counts.index, father_counts.values, label='Father')\n",
    "plt.xlabel('Book ID')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.title('Evolution of Word Occurrences (Holy, Son, Father)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualisation en nuage de mots de ceux les plus utilisés entre ancien et nouveau testament"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Filtrage la base de données pour obtenir le texte de l'Ancien Testament\n",
    "old_testament_text = ' '.join(df[df['testament'] == 'Old']['text'])\n",
    "\n",
    "# Générer un nuage de mots pour l'Ancien Testament\n",
    "wordcloud_old = WordCloud(width=800, height=400, max_font_size=150).generate(old_testament_text)\n",
    "\n",
    "# Filtrer le dataframe pour obtenir le texte du Nouveau Testament\n",
    "new_testament_text = ' '.join(df[df['testament'] == 'New']['text'])\n",
    "\n",
    "# Générer un nuage de mots pour le Nouveau Testament\n",
    "wordcloud_new = WordCloud(width=800, height=400, max_font_size=150).generate(new_testament_text)\n",
    "\n",
    "# Plot the word clouds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_old, interpolation='bilinear')\n",
    "plt.title('Nuage de mots - Ancien Testament')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_new, interpolation='bilinear')\n",
    "plt.title('Nuage de mots - Nouveau Testament')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-gramm / Bi-gramm / Tri-gramm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fonction pour obtenir les n-grammes les plus fréquents\n",
    "def get_most_frequent_ngrams(text, n, top_k):\n",
    "    # Tokeniser le texte en mots\n",
    "    words = text.split()\n",
    "    \n",
    "    # Générer les n-grammes\n",
    "    ngrams_list = list(ngrams(words, n))\n",
    "    \n",
    "    # Compter la fréquence de chaque n-gramme\n",
    "    ngrams_freq = Counter(ngrams_list)\n",
    "    \n",
    "    # Obtenir les k n-grammes les plus fréquents\n",
    "    top_ngrams = ngrams_freq.most_common(top_k)\n",
    "    \n",
    "    return top_ngrams\n",
    "\n",
    "# Obtenir les bigrammes les plus fréquents dans l'Ancien Testament\n",
    "old_testament_bigrams = get_most_frequent_ngrams(old_testament_text, 2, 10)\n",
    "\n",
    "# Obtenir les bigrammes les plus fréquents dans le Nouveau Testament\n",
    "new_testament_bigrams = get_most_frequent_ngrams(new_testament_text, 2, 10)\n",
    "\n",
    "# Tracer les graphiques à barres pour les bigrammes de l'Ancien et du Nouveau Testament\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Bigrammes de l'Ancien Testament\n",
    "axes[0].barh([str(bigram) for bigram, frequency in old_testament_bigrams], [frequency for bigram, frequency in old_testament_bigrams], color='blue')\n",
    "axes[0].set_title('Top 10 des bigrammes dans l\\'Ancien Testament')\n",
    "axes[0].set_xlabel('Fréquence')\n",
    "\n",
    "# Bigrammes du Nouveau Testament\n",
    "axes[1].barh([str(bigram) for bigram, frequency in new_testament_bigrams], [frequency for bigram, frequency in new_testament_bigrams], color='green')\n",
    "axes[1].set_title('Top 10 des bigrammes dans le Nouveau Testament')\n",
    "axes[1].set_xlabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fonction pour obtenir les n-grammes les plus fréquents\n",
    "def get_most_frequent_ngrams(text, n, top_k):\n",
    "    # Tokeniser le texte en mots\n",
    "    words = text.split()\n",
    "    \n",
    "    # Générer les n-grammes\n",
    "    ngrams_list = list(ngrams(words, n))\n",
    "    \n",
    "    # Compter la fréquence de chaque n-gramme\n",
    "    ngrams_freq = Counter(ngrams_list)\n",
    "    \n",
    "    # Obtenir les k n-grammes les plus fréquents\n",
    "    top_ngrams = ngrams_freq.most_common(top_k)\n",
    "    \n",
    "    return top_ngrams\n",
    "\n",
    "# Obtenir les trigrammes les plus fréquents dans l'Ancien Testament\n",
    "old_testament_trigrams = get_most_frequent_ngrams(old_testament_text, 3, 10)\n",
    "\n",
    "# Obtenir les trigrammes les plus fréquents dans le Nouveau Testament\n",
    "new_testament_trigrams = get_most_frequent_ngrams(new_testament_text, 3, 10)\n",
    "\n",
    "# Trier les trigrammes par fréquence en ordre décroissant\n",
    "old_testament_trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "new_testament_trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Tracer les graphiques à barres pour les trigrammes de l'Ancien et du Nouveau Testament\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Trigrammes de l'Ancien Testament\n",
    "axes[0].barh([str(trigram) for trigram, frequency in old_testament_trigrams], [frequency for trigram, frequency in old_testament_trigrams], color='blue')\n",
    "axes[0].set_title('Top 10 des trigrammes dans l\\'Ancien Testament')\n",
    "axes[0].set_xlabel('Fréquence')\n",
    "\n",
    "# Trigrammes du Nouveau Testament\n",
    "axes[1].barh([str(trigram) for trigram, frequency in new_testament_trigrams], [frequency for trigram, frequency in new_testament_trigrams], color='green')\n",
    "axes[1].set_title('Top 10 des trigrammes dans le Nouveau Testament')\n",
    "axes[1].set_xlabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BOW - Corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Utilisation de CountVectorizer pour obtenir les caractéristiques du BOW\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(df['text_cleaned'])\n",
    "\n",
    "# Mots uniques dans le corpus\n",
    "vocab = cv.get_feature_names_out()\n",
    "\n",
    "# Affichage des vecteurs de caractéristiques des documents dans un DataFrame\n",
    "bible_df_feature_vectors = pd.DataFrame(cv_matrix.toarray(), columns=vocab)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "bible_df_feature_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Aggrégation des versets par livre pour effecter une analyse par livre\n",
    "\n",
    "books = {}\n",
    "for verse in range(len(df)):\n",
    "    current_book = df['book_id'].iloc[verse]\n",
    "    if(current_book in books):\n",
    "        books[current_book] = str(books[current_book]) + str(df['text_cleaned'].iloc[verse]) + ' '\n",
    "    else: \n",
    "        books[current_book] = str(df['text_cleaned'].iloc[verse]) + ' '\n",
    "\n",
    "books_list = list(books.values())\n",
    "books_df = pd.DataFrame(books_list, columns=['corpus'])\n",
    "\n",
    "del books_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Création d'un objet TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit du TfidfVectorizer sur la colonne 'text_cleaned' des livres échantillonnés\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(books_df['corpus'])\n",
    "\n",
    "# Vocabulaire\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Création d'une liste pour stocker les 3 principaux termes TF-IDF pour chaque livre\n",
    "top_tfidf_terms_list = []\n",
    "\n",
    "# Pour chaque livre échantillonné, obtention des  3 principaux termes TF-IDF\n",
    "for idx, book in enumerate(books_df['corpus']):\n",
    "    tfidf_scores = tfidf_matrix[idx, :].toarray().flatten()\n",
    "    \n",
    "    # Création d'un dictionnaire pour stocker le terme et son score TF-IDF\n",
    "    tfidf_dict = {term: round(score, 2) for term, score in zip(vocab, tfidf_scores)}\n",
    "    \n",
    "    # Tri des termes par score TF-IDF, et obtention des trois premiers\n",
    "    sorted_tfidf_terms = sorted(tfidf_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    # Ajout des résultats à la liste\n",
    "    top_tfidf_terms_list.append({'Book': idx, 'Top 3 Terms': sorted_tfidf_terms})\n",
    "\n",
    "# Création d'un DataFrame à partir de la liste\n",
    "top_tfidf_terms_df = pd.DataFrame(top_tfidf_terms_list)\n",
    "\n",
    "top_tfidf_terms_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LDA "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le dictionnaire est un mappage entre les mots et leurs identifiants entiers,\n",
    "Le corpus est une liste de documents représentés sous forme d'un BoW."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "documents = df[\"text_cleaned\"].apply(lambda x: x.split(' '))\n",
    "\n",
    "# Création d'un dictionnaire\n",
    "id2word = corpora.Dictionary(documents)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in documents]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialisation des variables pour le meilleur modèle\n",
    "best_coherence = -1\n",
    "best_lda_model = None\n",
    "best_num_topics = 0\n",
    "\n",
    "# Esssai de différents nombres de topics\n",
    "for num_topics in range(2, 10):\n",
    "    # Construction du modèle LDA\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, random_state=42, passes=10, alpha=\"auto\", per_word_topics=True)\n",
    "\n",
    "    # Calcul du score de cohérence\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=documents, dictionary=id2word, coherence=\"c_v\")\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    # Pour chaque nombre de topic, affichage du score de cohérence lié\n",
    "    print(f\"Nombre de topics: {num_topics}, score de cohérence: {coherence_lda}\")\n",
    "\n",
    "    # Stockage et mise à jour du meilleur modèle, si ce dernier est plus cohérent\n",
    "    if coherence_lda > best_coherence:\n",
    "        best_coherence = coherence_lda\n",
    "        best_lda_model = lda_model\n",
    "        best_num_topics = num_topics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Affichage du meilleur nombre de topics\n",
    "print(f\"\\nBest Number of Topics: {best_num_topics}\")\n",
    "\n",
    "# Imprimer les mots-clés pour chaque sujet dans le meilleur modèle\n",
    "pprint(best_lda_model.print_topics())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons décidé de calculer la cohérence pour différents nombres de sujets et à choisir le modèle qui donne la cohérence la plus élevée. Ici 3 est indiqué comme le meilleur nombres de Topics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Afficher la visualisation du meilleur modèle\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(best_lda_model, corpus, id2word)\n",
    "pyLDAvis.display(vis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_cleaned'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_tfidf = pca.fit_transform(tfidf_matrix.toarray())\n",
    "reduced_tfidf.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scores = []  # ici on va stocker les scores\n",
    "cluster_range = range(1, 10)  # et ici le nombre de clusters que l'on veut tester (de 1 à 10)\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=100)\n",
    "    kmeans.fit(reduced_tfidf)\n",
    "    scores.append(kmeans.inertia_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimal_k = 3  # Replace 3 with the actual optimal number of clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=100)\n",
    "clusters = kmeans.fit_predict(reduced_tfidf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_tfidf[:, 0], reduced_tfidf[:, 1], c=clusters, cmap='rainbow')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Clusters des livres de la Bible')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis avec NLTK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialiser l'analyseur d'intensité des sentiments\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Définir une fonction pour calculer le score de sentiment pour un verset donné\n",
    "def get_sentiment_score(verse):\n",
    "    # Calculate sentiment score\n",
    "    sentiment_score = sia.polarity_scores(verse)['compound']\n",
    "    return sentiment_score\n",
    "\n",
    "# Apply the sentiment analysis function to the 'text_cleaned' column in your DataFrame\n",
    "df['sentiment_score'] = df['text_cleaned'].apply(get_sentiment_score)\n",
    "\n",
    "# Categorize sentiment based on the sentiment score\n",
    "df['sentiment'] = df['sentiment_score'].apply(lambda score: 'positive' if score > 0 else 'negative' if score < 0 else 'neutral')\n",
    "\n",
    "df[['text_cleaned', 'sentiment_score', 'sentiment']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Regrouper les données par book_id et sentiment\n",
    "grouped_data = df.groupby(['book_id', 'sentiment']).size().unstack()\n",
    "\n",
    "# Calculer la proportion de chaque sentiment par book_id\n",
    "proportional_data = grouped_data.div(grouped_data.sum(axis=1), axis=0)\n",
    "proportional_data.plot(figsize=(12, 6), kind='bar', stacked=True, width=0.8, color=['#FF4D4D', '#66CC66', '#4DA6FF'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
